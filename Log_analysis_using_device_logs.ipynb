{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyNtXOXGBVCmZBKQo+6T+p6P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ananya-Nair20/predective-log-analysis/blob/main/Log_analysis_using_device_logs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcgO0vf7B-zQ",
        "outputId": "b8a4df64-41ec-4381-9e1a-9dab597aeefe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive # Import the 'drive' object\n",
        "\n",
        "#Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Cleaning"
      ],
      "metadata": {
        "id": "KvR3TTlrCPNM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: I want to read multiple files and clean, preprocess multiple files and multiple columns. The columns are empolyee_name, user_id, email, role, projects,buisness_unit, function_unit, department, team, supervisor. write a code according to clean and preprocess 20+ files at the same time and all these columns in the same block of code\n",
        "\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, when, regexp_replace, trim\n",
        "import os\n",
        "\n",
        "\n",
        "# Define the path to your data files\n",
        "data_dir = f\"/content/drive/MyDrive/device_logs\"\n",
        "# Replace with the actual path\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder.appName(\"DataCleaning\").getOrCreate()\n",
        "\n",
        "# Columns to clean and preprocess\n",
        "columns_to_clean = [\"id\", \"date\", \"user\", \"pc\", \"file_tree\",\n",
        "                    \"activity\"]\n",
        "\n",
        "# Function to clean a single DataFrame\n",
        "def clean_dataframe(df_spark):\n",
        "  for col_name in columns_to_clean:\n",
        "    # Handling Missing Values: Fill nulls with empty string for string columns\n",
        "    if df_spark.schema[col_name].dataType == 'string':\n",
        "        df_spark = df_spark.na.fill('', subset=[col_name])\n",
        "    else:\n",
        "        df_spark = df_spark.na.fill(0, subset=[col_name])  # Or appropriate value for the column type\n",
        "\n",
        "    # Removing Special Characters/Cleaning Strings\n",
        "    #df_spark = df_spark.withColumn(col_name, regexp_replace(col(col_name), \"[^a-zA-Z0-9 ]\", \"\"))\n",
        "\n",
        "    # Trimming Whitespace\n",
        "    df_spark = df_spark.withColumn(col_name, trim(col(col_name)))\n",
        "\n",
        "  # Removing Duplicates\n",
        "  #df_spark = df_spark.dropDuplicates()\n",
        "\n",
        "  return df_spark\n",
        "\n",
        "# Loop through all files in the directory\n",
        "for filename in os.listdir(data_dir):\n",
        "    if filename.endswith(\".csv\"): #process only csv files\n",
        "        file_path = os.path.join(data_dir, filename)\n",
        "        print(f\"Processing file: {filename}\")\n",
        "\n",
        "        try:\n",
        "            # Load each CSV file into a Spark DataFrame\n",
        "            df_spark = spark.read.csv(file_path, header=True, inferSchema=True)\n",
        "\n",
        "            # Clean and preprocess the DataFrame\n",
        "            df_spark = clean_dataframe(df_spark)\n",
        "\n",
        "            # Output path for the cleaned file\n",
        "            output_file_path = os.path.join('/content/drive/MyDrive/cleaned', filename.replace('.csv', '_clean.csv'))\n",
        "\n",
        "            # Save the cleaned data\n",
        "            df_spark.write.csv(output_file_path, header=True, mode='overwrite')\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {filename}: {e}\")\n",
        "\n",
        "print(\"Data cleaning process completed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsyskno9CK4D",
        "outputId": "18dd38ee-889b-41b6-a2f5-8d0d3ba9f524"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing file: device.csv\n",
            "Data cleaning process completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: write a code to save this preprocessed data into a checkpoint so i wouldn't have to run the previous cells again and again\n",
        "\n",
        "import os\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize SparkSession (if not already initialized)\n",
        "spark = SparkSession.builder.appName(\"DataCheckpoint\").getOrCreate()\n",
        "\n",
        "# Define checkpoint directory\n",
        "checkpoint_dir = \"/content/drive/MyDrive/checkpoint\"  # Replace with your desired checkpoint directory\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "# Load the preprocessed data from the cleaned CSV files.  Assuming the cleaned data is in\n",
        "# /content/drive/MyDrive/cleaned/\n",
        "cleaned_data_dir = \"/content/drive/MyDrive/cleaned\"\n",
        "\n",
        "# Check if checkpoint exists\n",
        "if not os.path.exists(os.path.join(checkpoint_dir, \"_SUCCESS\")):\n",
        "    print(\"Checkpoint does not exist. Loading and saving data.\")\n",
        "\n",
        "    # Initialize an empty list to store DataFrames\n",
        "    all_dfs = []\n",
        "\n",
        "    for filename in os.listdir(cleaned_data_dir):\n",
        "        if filename.endswith(\"_clean.csv\"):\n",
        "            file_path = os.path.join(cleaned_data_dir, filename)\n",
        "            try:\n",
        "              df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
        "              all_dfs.append(df)\n",
        "            except Exception as e:\n",
        "              print(f\"Error loading file {filename}: {e}\")\n",
        "\n",
        "    #Combine all dataframes together\n",
        "    if all_dfs:\n",
        "      combined_df = all_dfs[0]\n",
        "      for i in range(1, len(all_dfs)):\n",
        "          combined_df = combined_df.union(all_dfs[i])\n",
        "\n",
        "      # Write the combined dataframe to the checkpoint directory\n",
        "      combined_df.write.parquet(checkpoint_dir, mode=\"overwrite\")\n",
        "      # Create a success file to indicate completion\n",
        "      with open(os.path.join(checkpoint_dir, \"_SUCCESS\"), \"w\") as f:\n",
        "          pass\n",
        "\n",
        "else:\n",
        "    print(\"Checkpoint exists. Loading data from checkpoint.\")\n",
        "    # Load the data from the checkpoint directory directly\n",
        "    combined_df = spark.read.parquet(checkpoint_dir)\n",
        "\n",
        "\n",
        "# Now combined_df holds your preprocessed data, loaded from the checkpoint if it exists\n",
        "# or created from the cleaned data if not\n",
        "print(\"Data successfully loaded\")\n",
        "# Continue with your analysis using combined_df\n",
        "# Example\n",
        "#combined_df.show(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxK4XpxWBxKi",
        "outputId": "06572b50-945f-4639-a429-c7274d1d6f74"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint exists. Loading data from checkpoint.\n",
            "Data successfully loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FEATURE ENGINEERING:\n",
        "\n",
        ">\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8m7DDoxXIp9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import countDistinct, size, split, date_format, to_timestamp, hour, dayofweek, count, when, lit\n",
        "from pyspark.sql.types import StringType, TimestampType\n",
        "\n",
        "# Assuming 'df_spark' is your cleaned DataFrame from the previous code.\n",
        "\n",
        "# 1. Activity Count per User (per hour)\n",
        "df_spark = df_spark.withColumn(\"timestamp\", to_timestamp(col(\"date\"), \"dd/MM/yyyy HH:mm:ss\"))\n",
        "df_spark = df_spark.withColumn(\"date_only\", date_format(col(\"timestamp\"), \"dd/MM/yyyy\").cast(StringType()))\n",
        "df_spark = df_spark.withColumn(\"time_only\", date_format(col(\"timestamp\"), \"HH:mm:ss\").cast(StringType()))\n",
        "\n",
        "# Filter out rows with null timestamps before grouping\n",
        "df_spark = df_spark.filter(col(\"timestamp\").isNotNull())\n",
        "\n",
        "activity_counts = df_spark.groupBy(\"user\", \"date_only\", \"time_only\").agg(count(\"*\").alias(\"activity_count_per_hour\"))\n",
        "df_spark = df_spark.join(activity_counts, [\"user\", \"date_only\", \"time_only\"], \"left\")\n",
        "\n",
        "# Select necessary columns to avoid duplicates\n",
        "df_spark = df_spark.select(\"id\", \"date\", \"user\", \"pc\", \"file_tree\", \"activity\", \"timestamp\", \"date_only\", \"time_only\", \"activity_count_per_hour\")\n",
        "\n",
        "# 2. Unique PCs Accessed per User\n",
        "unique_pcs = df_spark.groupBy(\"user\").agg(countDistinct(\"pc\").alias(\"unique_pcs_accessed\"))\n",
        "df_spark = df_spark.join(unique_pcs, \"user\", \"left\")\n",
        "\n",
        "# Select necessary columns again\n",
        "df_spark = df_spark.select(\"id\", \"date\", \"user\", \"pc\", \"file_tree\", \"activity\", \"timestamp\", \"date_only\", \"time_only\", \"activity_count_per_hour\", \"unique_pcs_accessed\")\n",
        "\n",
        "# 3. File Path Depth - Handling empty paths\n",
        "df_spark = df_spark.withColumn(\"file_path_depth\", when(col(\"file_tree\") != '', size(split(col(\"file_tree\"), \"/\"))).otherwise(lit(0)))\n",
        "\n",
        "# 4. Activity Frequency (per day) - Ensuring date_only is in the correct format for join\n",
        "activity_frequency = df_spark.groupBy(\"user\", \"date_only\").agg(count(\"*\").alias(\"activity_frequency_per_day\"))\n",
        "df_spark = df_spark.join(activity_frequency, [\"user\", \"date_only\"], \"left\")\n",
        "\n",
        "# Select necessary columns\n",
        "df_spark = df_spark.select(\"id\", \"date\", \"user\", \"pc\", \"file_tree\", \"activity\", \"timestamp\", \"date_only\", \"time_only\", \"activity_count_per_hour\", \"unique_pcs_accessed\", \"file_path_depth\", \"activity_frequency_per_day\")\n",
        "\n",
        "# 5. Day of Week / Hour of Day\n",
        "df_spark = df_spark.withColumn(\"day_of_week\", dayofweek(\"timestamp\"))\n",
        "df_spark = df_spark.withColumn(\"hour_of_day\", hour(\"timestamp\"))\n",
        "\n",
        "# Show the first 20 rows of the dataframe\n",
        "df_spark.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdjW4aCTBgZ8",
        "outputId": "99af31ab-7236-463c-f035-5ed0ae815fc0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-------------------+-------+-------+--------------------+----------+-------------------+----------+---------+-----------------------+-------------------+---------------+--------------------------+-----------+-----------+\n",
            "|                  id|               date|   user|     pc|           file_tree|  activity|          timestamp| date_only|time_only|activity_count_per_hour|unique_pcs_accessed|file_path_depth|activity_frequency_per_day|day_of_week|hour_of_day|\n",
            "+--------------------+-------------------+-------+-------+--------------------+----------+-------------------+----------+---------+-----------------------+-------------------+---------------+--------------------------+-----------+-----------+\n",
            "|{W0H4-S5OD03FD-56...|01/02/2010 07:48:30|ANC1950|PC-4921|R:\\;R:\\23svS11;R:...|   Connect|2010-02-01 07:48:30|01/02/2010| 07:48:30|                      1|                  1|              1|                         4|          2|          7|\n",
            "|{Y3V7-B9RC11WO-81...|10/01/2010 01:14:15|BMM0462|PC-8827|R:\\;R:\\65B8Vr4;R:...|   Connect|2010-01-10 01:14:15|10/01/2010| 01:14:15|                      1|                352|              1|                         8|          1|          1|\n",
            "|{Y0N3-X6HV55BA-14...|10/01/2010 01:16:06|BMM0462|PC-8827|                NULL|Disconnect|2010-01-10 01:16:06|10/01/2010| 01:16:06|                      1|                352|              0|                         8|          1|          1|\n",
            "|{M2J3-F1JZ92QP-71...|10/01/2010 01:10:38|IKP1047|PC-3951|      R:\\;R:\\IKP1047|   Connect|2010-01-10 01:10:38|10/01/2010| 01:10:38|                      1|                254|              1|                         6|          1|          1|\n",
            "|{N5K7-S2AI46KC-55...|10/01/2010 01:12:49|IKP1047|PC-3951|                NULL|Disconnect|2010-01-10 01:12:49|10/01/2010| 01:12:49|                      1|                254|              0|                         6|          1|          1|\n",
            "|{N0N4-S0KF96GO-98...|01/02/2010 07:41:02|ROR3483|PC-4365|R:\\;R:\\177Qcm8;R:...|   Connect|2010-02-01 07:41:02|01/02/2010| 07:41:02|                      1|                  1|              1|                        16|          2|          7|\n",
            "|{B6W4-L4VO73SE-09...|01/02/2010 07:46:34|ROR3483|PC-4365|                NULL|Disconnect|2010-02-01 07:46:34|01/02/2010| 07:46:34|                      1|                  1|              0|                        16|          2|          7|\n",
            "|{X6P2-R0OW84AX-41...|10/01/2010 01:53:09|JCB3054|PC-3600|R:\\;R:\\11V6vR4;R:...|   Connect|2010-01-10 01:53:09|10/01/2010| 01:53:09|                      1|                383|              1|                         4|          1|          1|\n",
            "|{S8L0-O6QQ15NL-06...|01/02/2010 07:33:28|GNT0221|PC-6427|      R:\\;R:\\GNT0221|   Connect|2010-02-01 07:33:28|01/02/2010| 07:33:28|                      1|                  1|              1|                        18|          2|          7|\n",
            "|{W8G8-O4IR40KC-62...|01/02/2010 07:43:29|GNT0221|PC-6427|                NULL|Disconnect|2010-02-01 07:43:29|01/02/2010| 07:43:29|                      1|                  1|              0|                        18|          2|          7|\n",
            "|{X0M4-W4KD87WB-50...|10/01/2010 02:54:48|DLM2440|PC-3670|                NULL|Disconnect|2010-01-10 02:54:48|10/01/2010| 02:54:48|                      1|                377|              0|                         8|          1|          2|\n",
            "|{C7F1-G7LE60RU-24...|01/02/2010 07:22:42|JKS2444|PC-6961|      R:\\;R:\\JKS2444|   Connect|2010-02-01 07:22:42|01/02/2010| 07:22:42|                      1|                  1|              1|                        18|          2|          7|\n",
            "|{U0F1-R1FX27FM-69...|01/02/2010 07:33:55|JKS2444|PC-6961|                NULL|Disconnect|2010-02-01 07:33:55|01/02/2010| 07:33:55|                      1|                  1|              0|                        18|          2|          7|\n",
            "|{Z2Q8-K3AV28BE-93...|01/02/2010 07:17:18|SDH2394|PC-5849|R:\\;R:\\22B5gX4;R:...|   Connect|2010-02-01 07:17:18|01/02/2010| 07:17:18|                      1|                  1|              1|                        14|          2|          7|\n",
            "|{T3E4-I2OZ53JE-92...|10/01/2010 02:55:38|CHB1062|PC-2803|                NULL|Disconnect|2010-01-10 02:55:38|10/01/2010| 02:55:38|                      1|                404|              0|                         2|          1|          2|\n",
            "|{A7X4-Y3DM48XA-73...|10/01/2010 00:49:44|LEB1764|PC-2341|R:\\;R:\\54C2TG4;R:...|   Connect|2010-01-10 00:49:44|10/01/2010| 00:49:44|                      1|                385|              1|                         2|          1|          0|\n",
            "|{F7Z4-U0BJ54IA-11...|01/02/2010 07:40:11|RCT1697|PC-5770|R:\\;R:\\488TX69;R:...|   Connect|2010-02-01 07:40:11|01/02/2010| 07:40:11|                      1|                  1|              1|                        18|          2|          7|\n",
            "|{M1J9-R5HR77HW-92...|01/02/2010 07:42:43|DJA0740|PC-7197|R:\\;R:\\36tgdN0;R:...|   Connect|2010-02-01 07:42:43|01/02/2010| 07:42:43|                      1|                  1|              1|                         2|          2|          7|\n",
            "|{V8D2-M8GY65KS-99...|10/01/2010 01:03:56|QAH0048|PC-4007|                NULL|Disconnect|2010-01-10 01:03:56|10/01/2010| 01:03:56|                      1|                378|              0|                         6|          1|          1|\n",
            "|{G7P6-Q6CY11UE-53...|01/02/2010 07:55:26|ANC1950|PC-4921|                NULL|Disconnect|2010-02-01 07:55:26|01/02/2010| 07:55:26|                      1|                  1|              0|                         4|          2|          7|\n",
            "+--------------------+-------------------+-------+-------+--------------------+----------+-------------------+----------+---------+-----------------------+-------------------+---------------+--------------------------+-----------+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: create a checkpoint for saving the previous output\n",
        "\n",
        "# Checkpoint the DataFrame\n",
        "checkpoint_dir_features = \"/content/drive/MyDrive/checkpoint_features\"  # Replace with your desired checkpoint directory\n",
        "os.makedirs(checkpoint_dir_features, exist_ok=True)\n",
        "\n",
        "# Check if checkpoint exists\n",
        "if not os.path.exists(os.path.join(checkpoint_dir_features, \"_SUCCESS\")):\n",
        "    print(\"Checkpoint for features does not exist. Saving data.\")\n",
        "\n",
        "    # Write the combined dataframe to the checkpoint directory\n",
        "    df_spark.write.parquet(checkpoint_dir_features, mode=\"overwrite\")\n",
        "    # Create a success file to indicate completion\n",
        "    with open(os.path.join(checkpoint_dir_features, \"_SUCCESS\"), \"w\") as f:\n",
        "        pass\n",
        "else:\n",
        "    print(\"Checkpoint for features exists. Loading data from checkpoint.\")\n",
        "    # Load the data from the checkpoint directory directly\n",
        "    df_spark = spark.read.parquet(checkpoint_dir_features)\n",
        "\n",
        "print(\"Data successfully checkpointed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ae4CA59t51tw",
        "outputId": "657c4556-e7ad-4257-e90a-fd0f3c5627b0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint for features exists. Loading data from checkpoint.\n",
            "Data successfully checkpointed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import rand\n",
        "\n",
        "# Assuming df_spark is your DataFrame with features\n",
        "\n",
        "# Split the data into training, validation, and testing sets (80%, 10%, 10%)\n",
        "train_df, test_df = df_spark.randomSplit([0.8, 0.2], seed=42)\n",
        "test_df, val_df = test_df.randomSplit([0.5,0.5], seed=42)\n",
        "\n",
        "# Show the sizes of the splits\n",
        "print(f\"Train set size: {train_df.count()}\")\n",
        "print(f\"Validation set size: {val_df.count()}\")\n",
        "print(f\"Test set size: {test_df.count()}\")\n",
        "\n",
        "# Save the splits to parquet files (optional)\n",
        "# train_df.write.parquet(\"/content/drive/MyDrive/splits/train\", mode=\"overwrite\")\n",
        "# val_df.write.parquet(\"/content/drive/MyDrive/splits/val\", mode=\"overwrite\")\n",
        "# test_df.write.parquet(\"/content/drive/MyDrive/splits/test\", mode=\"overwrite\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cBBhwLCRTPzs",
        "outputId": "0f462fcc-65a3-4c73-a174-f5ecd282a948"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set size: 1241822\n",
            "Validation set size: 154616\n",
            "Test set size: 155390\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: write a code to create a checkpoint to save the above code\n",
        "\n",
        "# Assuming df_spark is your DataFrame with features from the previous code.\n",
        "\n",
        "# Checkpoint the DataFrame\n",
        "checkpoint_dir_final = \"/content/drive/MyDrive/checkpoint_final\"  # Replace with your desired checkpoint directory\n",
        "os.makedirs(checkpoint_dir_final, exist_ok=True)\n",
        "\n",
        "# Check if checkpoint exists\n",
        "if not os.path.exists(os.path.join(checkpoint_dir_final, \"_SUCCESS\")):\n",
        "    print(\"Checkpoint for final data does not exist. Saving data.\")\n",
        "\n",
        "    # Write the combined dataframe to the checkpoint directory\n",
        "    df_spark.write.parquet(checkpoint_dir_final, mode=\"overwrite\")\n",
        "    # Create a success file to indicate completion\n",
        "    with open(os.path.join(checkpoint_dir_final, \"_SUCCESS\"), \"w\") as f:\n",
        "        pass\n",
        "else:\n",
        "    print(\"Checkpoint for final data exists. Loading data from checkpoint.\")\n",
        "    # Load the data from the checkpoint directory directly\n",
        "    df_spark = spark.read.parquet(checkpoint_dir_final)\n",
        "\n",
        "print(\"Data successfully checkpointed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "muRN-v-Y6d-B",
        "outputId": "ed7da881-ed89-45b9-b378-30b7f25a2380"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint for final data exists. Loading data from checkpoint.\n",
            "Data successfully checkpointed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "# Assuming 'df_spark' is defined from the previous code.\n",
        "# Define features (excluding the target variable 'activity' and irrelevant columns)\n",
        "feature_cols = [\"activity_count_per_hour\", \"unique_pcs_accessed\", \"file_path_depth\", \"activity_frequency_per_day\", \"day_of_week\", \"hour_of_day\"]\n",
        "\n",
        "# Split the data into training, validation, and testing sets - This is important to recreate the splits\n",
        "train_data, temp_data = df_spark.randomSplit([0.7, 0.3], seed=42)  # 70% for training, 30% for temp\n",
        "validation_data, test_data = temp_data.randomSplit([0.66, 0.34], seed=42) # 30% of the total data (20% of the original)\n",
        "\n",
        "\n",
        "# Assemble features into a vector\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "train_data = assembler.transform(train_data)\n",
        "validation_data = assembler.transform(validation_data)\n",
        "test_data = assembler.transform(test_data)\n",
        "\n",
        "\n",
        "# Initialize the Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(labelCol=\"activity_frequency_per_day\", featuresCol=\"features\", numTrees=100)\n",
        "\n",
        "\n",
        "# Fit the model to the training data\n",
        "model = rf_classifier.fit(train_data)\n",
        "\n",
        "\n",
        "# Make predictions on the validation set\n",
        "predictions_validation = model.transform(validation_data)\n",
        "\n",
        "\n",
        "# Evaluate the model\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"activity_frequency_per_day\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy_validation = evaluator.evaluate(predictions_validation)\n",
        "\n",
        "print(f\"Validation Accuracy: {accuracy_validation}\")\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions_test = model.transform(test_data)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "accuracy_test = evaluator.evaluate(predictions_test)\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy_test}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "Ua3SsLhxFB2_",
        "outputId": "8994f029-f6da-44a4-f9bf-1fcc62f8efa9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IllegalArgumentException",
          "evalue": "activity_count_per_hour does not exist. Available: id, date, user, pc, file_tree, activity",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-c89b1823e102>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Assemble features into a vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0massembler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVectorAssembler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"features\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0massembler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0massembler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0massembler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    260\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIllegalArgumentException\u001b[0m: activity_count_per_hour does not exist. Available: id, date, user, pc, file_tree, activity"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: save the above code using a checkpoint so i don't have to keep running it again and again\n",
        "\n",
        "# Save the current state of the notebook using a checkpoint\n",
        "# This will create a checkpoint file in the specified directory\n",
        "# You can then restore the notebook to this state later.\n",
        "checkpoint_dir_final = \"/content/drive/MyDrive/checkpoint_final\"\n",
        "df_spark.write.parquet(checkpoint_dir_final, mode=\"overwrite\")\n",
        "# Create a success file to indicate completion\n",
        "with open(os.path.join(checkpoint_dir_final, \"_SUCCESS\"), \"w\") as f:\n",
        "    pass\n",
        "\n",
        "print(\"Checkpoint created successfully!\")\n"
      ],
      "metadata": {
        "id": "fsjI2Q5565bd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: write a code for hyperparameter tuning using gridsearch\n",
        "\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "# Create ParamGrid for hyperparameter tuning\n",
        "paramGrid = (ParamGridBuilder()\n",
        "             .addGrid(rf_classifier.numTrees, [50, 100, 200])\n",
        "             .addGrid(rf_classifier.maxDepth, [5, 10, 20])\n",
        "             .build())\n",
        "\n",
        "# Define evaluator\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"activity_frequency_per_day\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "\n",
        "# Create CrossValidator\n",
        "cv = CrossValidator(estimator=rf_classifier, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=3)\n",
        "\n",
        "\n",
        "# Run cross-validation\n",
        "cvModel = cv.fit(train_data)\n",
        "\n",
        "\n",
        "# Make predictions on the validation set using the best model\n",
        "bestModel = cvModel.bestModel\n",
        "predictions_validation = bestModel.transform(validation_data)\n",
        "\n",
        "# Evaluate the best model on the validation set\n",
        "accuracy_validation = evaluator.evaluate(predictions_validation)\n",
        "print(f\"Validation Accuracy (best model): {accuracy_validation}\")\n",
        "\n",
        "\n",
        "# Make predictions on the test set using the best model\n",
        "predictions_test = bestModel.transform(test_data)\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "accuracy_test = evaluator.evaluate(predictions_test)\n",
        "print(f\"Test Accuracy (best model): {accuracy_test}\")\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(f\"Best hyperparameters: {cvModel.bestModel.extractParamMap()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "GHML4c65aeR3",
        "outputId": "18d2bf1f-5253-4a60-9f09-63d845d9cbd1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'rf_classifier' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-22fb45db108d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Create ParamGrid for hyperparameter tuning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m paramGrid = (ParamGridBuilder()\n\u001b[0;32m----> 8\u001b[0;31m              \u001b[0;34m.\u001b[0m\u001b[0maddGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrf_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumTrees\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m              \u001b[0;34m.\u001b[0m\u001b[0maddGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrf_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxDepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m              .build())\n",
            "\u001b[0;31mNameError\u001b[0m: name 'rf_classifier' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter Tuning:"
      ],
      "metadata": {
        "id": "zddmUJtYJtNZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: write a code for hyperparameter tuning this isolation forest algorithm using gridsearchCV\n",
        "\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "# Create ParamGrid for hyperparameter tuning\n",
        "paramGrid = ParamGridBuilder() \\\n",
        "    .addGrid(rf_classifier.numTrees, [10, 50, 100]) \\\n",
        "    .addGrid(rf_classifier.maxDepth, [5, 10, 20]) \\\n",
        "    .addGrid(rf_classifier.impurity, ['gini', 'entropy']) \\\n",
        "    .build()\n",
        "\n",
        "# Define evaluator\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"activity_frequency_per_day\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "\n",
        "# Create CrossValidator\n",
        "crossval = CrossValidator(estimator=rf_classifier,\n",
        "                          estimatorParamMaps=paramGrid,\n",
        "                          evaluator=evaluator,\n",
        "                          numFolds=3)  # Adjust numFolds as needed\n",
        "\n",
        "# Run cross-validation\n",
        "cvModel = crossval.fit(train_data)\n",
        "\n",
        "# Make predictions on the validation set using the best model\n",
        "best_predictions_validation = cvModel.transform(validation_data)\n",
        "accuracy_validation = evaluator.evaluate(best_predictions_validation)\n",
        "print(f\"Validation Accuracy (after tuning): {accuracy_validation}\")\n",
        "\n",
        "# Make predictions on the test set using the best model\n",
        "best_predictions_test = cvModel.transform(test_data)\n",
        "accuracy_test = evaluator.evaluate(best_predictions_test)\n",
        "print(f\"Test Accuracy (after tuning): {accuracy_test}\")\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"Best hyperparameters:\", cvModel.bestModel.extractParamMap())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "J7TonlFoJ-dO",
        "outputId": "73059164-1ed1-4981-c11b-df499204040e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/lib/python3.11/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m                 \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    857\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-3c5f4420aaa0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Run cross-validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mcvModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrossval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Make predictions on the validation set using the best model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             raise TypeError(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/ml/tuning.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    845\u001b[0m                 \u001b[0m_parallelFitTasks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meva\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollectSubModelsParam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m             )\n\u001b[0;32m--> 847\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubModel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimap_unordered\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m                 \u001b[0mmetrics_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcollectSubModelsParam\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    859\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 861\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    862\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m                     \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}